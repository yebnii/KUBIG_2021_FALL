{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "모델링_tpot_iqr.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "565dfd640514464d844fa654fb0df44b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e599ac7d1ffc47fb938d396cd6c5ff4f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a06e35983ef44ab38bb03efdd289f453",
              "IPY_MODEL_c8439018d511479ab7b5936ee6dafda4",
              "IPY_MODEL_3aa73acc28e5442dbadf939ac67086ac"
            ]
          }
        },
        "e599ac7d1ffc47fb938d396cd6c5ff4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a06e35983ef44ab38bb03efdd289f453": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f5333a89b7e44a1ab4f815e2a650e004",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Optimization Progress: ",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6e05c65d8ec745e487a503f9a1a3b5c1"
          }
        },
        "c8439018d511479ab7b5936ee6dafda4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b31eecc2a65e4301b09c983493c357d6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 10100,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 10100,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ce34c132737e4a64acda45afc0578412"
          }
        },
        "3aa73acc28e5442dbadf939ac67086ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_78bc018ebc804d2791b1438a67b46972",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 10101/? [6:08:43&lt;00:00,  1.82s/pipeline]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_afe8fcb8e7c6401ab30ba05576d55380"
          }
        },
        "f5333a89b7e44a1ab4f815e2a650e004": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6e05c65d8ec745e487a503f9a1a3b5c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b31eecc2a65e4301b09c983493c357d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ce34c132737e4a64acda45afc0578412": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "78bc018ebc804d2791b1438a67b46972": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "afe8fcb8e7c6401ab30ba05576d55380": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0dCTfqy_d86"
      },
      "source": [
        "# IQR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "telgPMJc-hIQ"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pandas import Series"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dngvMxsYnlto",
        "outputId": "db8271e2-3b4d-4ce3-c84a-5c129a3da7fa"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uo-ef8IU-OCw"
      },
      "source": [
        "X_train=pd.read_csv('/content/drive/MyDrive/KUBIG/ML 2021-SUMMER/따릉이/train_X_iqr.csv')\n",
        "y_train=pd.read_csv('/content/drive/MyDrive/KUBIG/ML 2021-SUMMER/따릉이/train_Y_iqr.csv')\n",
        "X_test=pd.read_csv('/content/drive/MyDrive/KUBIG/ML 2021-SUMMER/따릉이/test_X_iqr.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7uUwj7D-e5K"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTAbEayK-wlI",
        "outputId": "4f936dca-dca4-4852-d488-211153020a21"
      },
      "source": [
        "cols=X_train.columns.to_list()\n",
        "list(enumerate(cols))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 'hour'),\n",
              " (1, 'hour_bef_temperature'),\n",
              " (2, 'hour_bef_precipitation'),\n",
              " (3, 'hour_bef_windspeed'),\n",
              " (4, 'hour_bef_humidity'),\n",
              " (5, 'hour_bef_visibility'),\n",
              " (6, 'hour_bef_ozone'),\n",
              " (7, 'hour_bef_pm10'),\n",
              " (8, 'hour_bef_pm2.5'),\n",
              " (9, 't_windspeed'),\n",
              " (10, 't_pm10'),\n",
              " (11, 't_pm2.5'),\n",
              " (12, 'hour_sin'),\n",
              " (13, 'hour_category'),\n",
              " (14, 'hour_category_1'),\n",
              " (15, 'hour_category_2')]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SkV7nY_HoZo"
      },
      "source": [
        "select=list(set(cols)-set(['hour_sin','hour_category','hour_category_1','hour_category_2','t_pm2.5','t_pm10','t_windspeed','hour_bef_visibility']))\n",
        "\n",
        "X_train_select=X_train[select]\n",
        "X_val_select=X_val[select]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N52zAA7yBbmV",
        "outputId": "dfc6e7b9-d11a-47f4-e781-1b1663579748"
      },
      "source": [
        "!pip install tpot"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tpot\n",
            "  Downloading TPOT-0.11.7-py3-none-any.whl (87 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▊                            | 10 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 20 kB 29.7 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 30 kB 27.9 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 40 kB 22.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 51 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 61 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 71 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 81 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 87 kB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.7/dist-packages (from tpot) (1.1.5)\n",
            "Collecting stopit>=1.1.1\n",
            "  Downloading stopit-1.1.2.tar.gz (18 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.7/dist-packages (from tpot) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.7/dist-packages (from tpot) (1.19.5)\n",
            "Collecting deap>=1.2\n",
            "  Downloading deap-1.3.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 20.8 MB/s \n",
            "\u001b[?25hCollecting update-checker>=0.16\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: tqdm>=4.36.1 in /usr/local/lib/python3.7/dist-packages (from tpot) (4.62.0)\n",
            "Collecting xgboost>=1.1.0\n",
            "  Downloading xgboost-1.4.2-py3-none-manylinux2010_x86_64.whl (166.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 166.7 MB 14 kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from tpot) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.7/dist-packages (from tpot) (1.0.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.2->tpot) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.2->tpot) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.2->tpot) (1.15.0)\n",
            "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from update-checker>=0.16->tpot) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2.10)\n",
            "Building wheels for collected packages: stopit\n",
            "  Building wheel for stopit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stopit: filename=stopit-1.1.2-py3-none-any.whl size=11952 sha256=02595b2d818786e57ab16ba5fc5ec337fb965392ce95f7f54e82481e960a2168\n",
            "  Stored in directory: /root/.cache/pip/wheels/e2/d2/79/eaf81edb391e27c87f51b8ef901ecc85a5363dc96b8b8d71e3\n",
            "Successfully built stopit\n",
            "Installing collected packages: xgboost, update-checker, stopit, deap, tpot\n",
            "  Attempting uninstall: xgboost\n",
            "    Found existing installation: xgboost 0.90\n",
            "    Uninstalling xgboost-0.90:\n",
            "      Successfully uninstalled xgboost-0.90\n",
            "Successfully installed deap-1.3.1 stopit-1.1.2 tpot-0.11.7 update-checker-0.18.0 xgboost-1.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "565dfd640514464d844fa654fb0df44b",
            "e599ac7d1ffc47fb938d396cd6c5ff4f",
            "a06e35983ef44ab38bb03efdd289f453",
            "c8439018d511479ab7b5936ee6dafda4",
            "3aa73acc28e5442dbadf939ac67086ac",
            "f5333a89b7e44a1ab4f815e2a650e004",
            "6e05c65d8ec745e487a503f9a1a3b5c1",
            "b31eecc2a65e4301b09c983493c357d6",
            "ce34c132737e4a64acda45afc0578412",
            "78bc018ebc804d2791b1438a67b46972",
            "afe8fcb8e7c6401ab30ba05576d55380"
          ]
        },
        "id": "Amk-bvtoDYoD",
        "outputId": "f138b3f6-dc6a-43a3-b976-4e1ff00bd38a"
      },
      "source": [
        "from tpot import TPOTRegressor\n",
        "tpot = TPOTRegressor(n_jobs=-1, verbosity=3,periodic_checkpoint_folder=\"tpot_results_no.txt\")\n",
        "tpot.fit(X_train, y_train)\n",
        "print(tpot.score(X_val, y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30 operators have been imported by TPOT.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "565dfd640514464d844fa654fb0df44b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Optimization Progress:   0%|          | 0/10100 [00:00<?, ?pipeline/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 [08:35:42] ../src/learner.cc:599: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7f279a74833f]\n",
            "  [bt] (1) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1c4518) [0x7f279a87b518]\n",
            "  [bt] (2) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1d21a3) [0x7f279a8891a3]\n",
            "  [bt] (3) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1b9829) [0x7f279a870829]\n",
            "  [bt] (4) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f279a737ed0]\n",
            "  [bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f284951adae]\n",
            "  [bt] (6) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7f284951a71f]\n",
            "  [bt] (7) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x28c) [0x7f284972e5ac]\n",
            "  [bt] (8) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x109e3) [0x7f284972d9e3]\n",
            "\n",
            ".\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 1 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1610.2560058829074\tRandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=6, RandomForestRegressor__min_samples_split=11, RandomForestRegressor__n_estimators=100)\n",
            "\n",
            "-2\t-1565.7693233382206\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=17, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=14, ExtraTreesRegressor__n_estimators=100)\n",
            "Created new folder to save periodic pipeline: tpot_results_no.txt\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_1_idx_0_2021.09.01_08-37-02.py\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_1_idx_1_2021.09.01_08-37-02.py\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 2 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1501.124413370709\tRandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100)\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_2_idx_0_2021.09.01_08-38-48.py\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 56.\n",
            "\n",
            "Generation 3 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1501.124413370709\tRandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100)\n",
            "\n",
            "-2\t-1490.4031397520398\tRandomForestRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=12, RandomForestRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_3_idx_1_2021.09.01_08-41-35.py\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 [08:41:42] ../src/learner.cc:599: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7f279a74833f]\n",
            "  [bt] (1) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1c4518) [0x7f279a87b518]\n",
            "  [bt] (2) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1d21a3) [0x7f279a8891a3]\n",
            "  [bt] (3) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1b9829) [0x7f279a870829]\n",
            "  [bt] (4) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f279a737ed0]\n",
            "  [bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f284951adae]\n",
            "  [bt] (6) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7f284951a71f]\n",
            "  [bt] (7) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x28c) [0x7f284972e5ac]\n",
            "  [bt] (8) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x109e3) [0x7f284972d9e3]\n",
            "\n",
            ".\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "\n",
            "Generation 4 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1501.124413370709\tRandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100)\n",
            "\n",
            "-2\t-1488.293583511299\tRandomForestRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=44, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_4_idx_1_2021.09.01_08-44-41.py\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 [08:44:42] ../src/learner.cc:599: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7f279a74833f]\n",
            "  [bt] (1) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1c4518) [0x7f279a87b518]\n",
            "  [bt] (2) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1d21a3) [0x7f279a8891a3]\n",
            "  [bt] (3) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1b9829) [0x7f279a870829]\n",
            "  [bt] (4) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f279a737ed0]\n",
            "  [bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f284951adae]\n",
            "  [bt] (6) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7f284951a71f]\n",
            "  [bt] (7) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x28c) [0x7f284972e5ac]\n",
            "  [bt] (8) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x109e3) [0x7f284972d9e3]\n",
            "\n",
            ".\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "Invalid pipeline encountered. Skipping its evaluation.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 5 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1486.733878721434\tRandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6500000000000001, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100)\n",
            "\n",
            "-2\t-1458.6916337508221\tRandomForestRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=12, RandomForestRegressor__n_estimators=100)\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_5_idx_0_2021.09.01_08-47-14.py\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_5_idx_1_2021.09.01_08-47-14.py\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
            "\n",
            "Generation 6 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1482.998921065409\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=12, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-2\t-1458.6916337508221\tRandomForestRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=12, RandomForestRegressor__n_estimators=100)\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_6_idx_0_2021.09.01_08-49-50.py\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "\n",
            "Generation 7 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1482.998921065409\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=12, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-2\t-1458.6916337508221\tRandomForestRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=12, RandomForestRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 53.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "\n",
            "Generation 8 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1482.998921065409\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=12, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-2\t-1458.6916337508221\tRandomForestRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=12, RandomForestRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 53.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
            "\n",
            "Generation 9 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1482.998921065409\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=12, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-2\t-1448.8592745296774\tExtraTreesRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.05, ElasticNetCV__tol=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_9_idx_1_2021.09.01_08-58-22.py\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 [08:58:27] ../src/learner.cc:599: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7f279a74833f]\n",
            "  [bt] (1) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1c4518) [0x7f279a87b518]\n",
            "  [bt] (2) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1d21a3) [0x7f279a8891a3]\n",
            "  [bt] (3) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1b9829) [0x7f279a870829]\n",
            "  [bt] (4) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f279a737ed0]\n",
            "  [bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f284951adae]\n",
            "  [bt] (6) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7f284951a71f]\n",
            "  [bt] (7) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x28c) [0x7f284972e5ac]\n",
            "  [bt] (8) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x109e3) [0x7f284972d9e3]\n",
            "\n",
            ".\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 [08:58:34] ../src/learner.cc:599: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7f279a74833f]\n",
            "  [bt] (1) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1c4518) [0x7f279a87b518]\n",
            "  [bt] (2) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1d21a3) [0x7f279a8891a3]\n",
            "  [bt] (3) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1b9829) [0x7f279a870829]\n",
            "  [bt] (4) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f279a737ed0]\n",
            "  [bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f284951adae]\n",
            "  [bt] (6) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7f284951a71f]\n",
            "  [bt] (7) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x28c) [0x7f284972e5ac]\n",
            "  [bt] (8) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x109e3) [0x7f284972d9e3]\n",
            "\n",
            ".\n",
            "\n",
            "Generation 10 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1473.6822558493463\tRandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100)\n",
            "\n",
            "-2\t-1448.8592745296774\tExtraTreesRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.05, ElasticNetCV__tol=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_10_idx_0_2021.09.01_09-01-07.py\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 11 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1473.6822558493463\tRandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100)\n",
            "\n",
            "-2\t-1448.8592745296774\tExtraTreesRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.05, ElasticNetCV__tol=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 52.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 12 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1431.0518535722815\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_12_idx_0_2021.09.01_09-07-39.py\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "Invalid pipeline encountered. Skipping its evaluation.\n",
            "Invalid pipeline encountered. Skipping its evaluation.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 13 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1431.0518535722815\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1405.9890294282795\tRandomForestRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_13_idx_1_2021.09.01_09-11-16.py\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "\n",
            "Generation 14 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1431.0518535722815\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1405.9890294282795\tRandomForestRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 87.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "\n",
            "Generation 15 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1409.1159084260512\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1405.9890294282795\tRandomForestRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100)\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_15_idx_0_2021.09.01_09-18-59.py\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 73.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 75.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "\n",
            "Generation 16 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1409.1159084260512\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1405.9890294282795\tRandomForestRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 71.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "\n",
            "Generation 17 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1409.1159084260512\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1405.9890294282795\tRandomForestRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 [09:27:28] ../src/learner.cc:599: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7f279a74833f]\n",
            "  [bt] (1) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1c4518) [0x7f279a87b518]\n",
            "  [bt] (2) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1d21a3) [0x7f279a8891a3]\n",
            "  [bt] (3) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1b9829) [0x7f279a870829]\n",
            "  [bt] (4) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f279a737ed0]\n",
            "  [bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f284951adae]\n",
            "  [bt] (6) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7f284951a71f]\n",
            "  [bt] (7) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x28c) [0x7f284972e5ac]\n",
            "  [bt] (8) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x109e3) [0x7f284972d9e3]\n",
            "\n",
            ".\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 71.\n",
            "\n",
            "Generation 18 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1409.1159084260512\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1405.9890294282795\tRandomForestRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100)\n",
            "\n",
            "-5\t-1404.2067829703515\tExtraTreesRegressor(ZeroCount(ExtraTreesRegressor(MinMaxScaler(ZeroCount(input_matrix)), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=5, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_18_idx_2_2021.09.01_09-31-29.py\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=2 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
            "\n",
            "Generation 19 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1409.1159084260512\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1405.9890294282795\tRandomForestRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100)\n",
            "\n",
            "-5\t-1404.2067829703515\tExtraTreesRegressor(ZeroCount(ExtraTreesRegressor(MinMaxScaler(ZeroCount(input_matrix)), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=5, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "_pre_test decorator: _mate_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 [09:37:29] ../src/learner.cc:599: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7f279a74833f]\n",
            "  [bt] (1) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1c4518) [0x7f279a87b518]\n",
            "  [bt] (2) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1d21a3) [0x7f279a8891a3]\n",
            "  [bt] (3) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1b9829) [0x7f279a870829]\n",
            "  [bt] (4) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f279a737ed0]\n",
            "  [bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f284951adae]\n",
            "  [bt] (6) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7f284951a71f]\n",
            "  [bt] (7) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x28c) [0x7f284972e5ac]\n",
            "  [bt] (8) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x109e3) [0x7f284972d9e3]\n",
            "\n",
            ".\n",
            "\n",
            "Generation 20 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1409.1159084260512\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1405.9890294282795\tRandomForestRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100)\n",
            "\n",
            "-5\t-1404.2067829703515\tExtraTreesRegressor(ZeroCount(ExtraTreesRegressor(MinMaxScaler(ZeroCount(input_matrix)), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=5, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "\n",
            "Generation 21 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1409.1159084260512\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1405.9890294282795\tRandomForestRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100)\n",
            "\n",
            "-5\t-1404.2067829703515\tExtraTreesRegressor(ZeroCount(ExtraTreesRegressor(MinMaxScaler(ZeroCount(input_matrix)), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=5, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 [09:44:19] ../src/learner.cc:599: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7f279a74833f]\n",
            "  [bt] (1) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1c4518) [0x7f279a87b518]\n",
            "  [bt] (2) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1d21a3) [0x7f279a8891a3]\n",
            "  [bt] (3) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1b9829) [0x7f279a870829]\n",
            "  [bt] (4) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f279a737ed0]\n",
            "  [bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f284951adae]\n",
            "  [bt] (6) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7f284951a71f]\n",
            "  [bt] (7) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x28c) [0x7f284972e5ac]\n",
            "  [bt] (8) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x109e3) [0x7f284972d9e3]\n",
            "\n",
            ".\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "\n",
            "Generation 22 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1409.1159084260512\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1405.9890294282795\tRandomForestRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1374.3222117069338\tRandomForestRegressor(OneHotEncoder(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=7, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_22_idx_2_2021.09.01_09-48-27.py\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 23 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1409.1159084260512\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1405.9890294282795\tRandomForestRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1374.3222117069338\tRandomForestRegressor(OneHotEncoder(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=7, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 24 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1408.9842390475114\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1405.9890294282795\tRandomForestRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1374.3222117069338\tRandomForestRegressor(OneHotEncoder(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=7, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100)\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_24_idx_0_2021.09.01_09-55-38.py\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 71.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=2 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
            "\n",
            "Generation 25 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1408.9842390475114\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1391.0790506733715\tExtraTreesRegressor(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), CombineDFs(input_matrix, input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1374.3222117069338\tRandomForestRegressor(OneHotEncoder(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=7, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_25_idx_1_2021.09.01_09-58-18.py\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "\n",
            "Generation 26 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1408.9842390475114\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1380.274666273703\tRandomForestRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1374.3222117069338\tRandomForestRegressor(OneHotEncoder(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=7, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_26_idx_1_2021.09.01_10-01-22.py\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 95.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 [10:01:26] ../src/learner.cc:599: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7f279a74833f]\n",
            "  [bt] (1) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1c4518) [0x7f279a87b518]\n",
            "  [bt] (2) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1d21a3) [0x7f279a8891a3]\n",
            "  [bt] (3) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1b9829) [0x7f279a870829]\n",
            "  [bt] (4) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f279a737ed0]\n",
            "  [bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f284951adae]\n",
            "  [bt] (6) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7f284951a71f]\n",
            "  [bt] (7) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x28c) [0x7f284972e5ac]\n",
            "  [bt] (8) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x109e3) [0x7f284972d9e3]\n",
            "\n",
            ".\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "Invalid pipeline encountered. Skipping its evaluation.\n",
            "\n",
            "Generation 27 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1408.9842390475114\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1380.274666273703\tRandomForestRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1374.3222117069338\tRandomForestRegressor(OneHotEncoder(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=7, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 28 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1391.336714244799\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1379.1405366052657\tRandomForestRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1374.3222117069338\tRandomForestRegressor(OneHotEncoder(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=7, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100)\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_28_idx_0_2021.09.01_10-07-54.py\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_28_idx_1_2021.09.01_10-07-54.py\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "\n",
            "Generation 29 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1391.336714244799\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1379.1405366052657\tRandomForestRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1374.3222117069338\tRandomForestRegressor(OneHotEncoder(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=7, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "\n",
            "Generation 30 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1391.336714244799\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1379.1405366052657\tRandomForestRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1369.283585290283\tRandomForestRegressor(CombineDFs(OneHotEncoder(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=7, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_30_idx_2_2021.09.01_10-19-11.py\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _mate_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 [10:19:18] ../src/learner.cc:599: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7f279a74833f]\n",
            "  [bt] (1) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1c4518) [0x7f279a87b518]\n",
            "  [bt] (2) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1d21a3) [0x7f279a8891a3]\n",
            "  [bt] (3) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1b9829) [0x7f279a870829]\n",
            "  [bt] (4) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f279a737ed0]\n",
            "  [bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f284951adae]\n",
            "  [bt] (6) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7f284951a71f]\n",
            "  [bt] (7) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x28c) [0x7f284972e5ac]\n",
            "  [bt] (8) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x109e3) [0x7f284972d9e3]\n",
            "\n",
            ".\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 31 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1391.336714244799\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1379.1405366052657\tRandomForestRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1369.283585290283\tRandomForestRegressor(CombineDFs(OneHotEncoder(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=7, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 75.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 78.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 64.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
            "\n",
            "Generation 32 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1376.5823378464063\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
            "\n",
            "-3\t-1369.283585290283\tRandomForestRegressor(CombineDFs(OneHotEncoder(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=7, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100)\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_32_idx_0_2021.09.01_10-26-03.py\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _mate_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _mate_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 [10:26:19] ../src/learner.cc:599: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7f279a74833f]\n",
            "  [bt] (1) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1c4518) [0x7f279a87b518]\n",
            "  [bt] (2) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1d21a3) [0x7f279a8891a3]\n",
            "  [bt] (3) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1b9829) [0x7f279a870829]\n",
            "  [bt] (4) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f279a737ed0]\n",
            "  [bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f284951adae]\n",
            "  [bt] (6) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7f284951a71f]\n",
            "  [bt] (7) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x28c) [0x7f284972e5ac]\n",
            "  [bt] (8) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x109e3) [0x7f284972d9e3]\n",
            "\n",
            ".\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "\n",
            "Generation 33 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1376.5823378464063\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
            "\n",
            "-3\t-1369.283585290283\tRandomForestRegressor(CombineDFs(OneHotEncoder(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=7, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1364.5885043305868\tRandomForestRegressor(OneHotEncoder(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.05, ExtraTreesRegressor__min_samples_leaf=6, ExtraTreesRegressor__min_samples_split=15, ExtraTreesRegressor__n_estimators=100), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=7, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_33_idx_2_2021.09.01_10-30-53.py\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 [10:31:03] ../src/learner.cc:599: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7f279a74833f]\n",
            "  [bt] (1) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1c4518) [0x7f279a87b518]\n",
            "  [bt] (2) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1d21a3) [0x7f279a8891a3]\n",
            "  [bt] (3) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1b9829) [0x7f279a870829]\n",
            "  [bt] (4) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f279a737ed0]\n",
            "  [bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f284951adae]\n",
            "  [bt] (6) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7f284951a71f]\n",
            "  [bt] (7) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x28c) [0x7f284972e5ac]\n",
            "  [bt] (8) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x109e3) [0x7f284972d9e3]\n",
            "\n",
            ".\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 100.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
            "\n",
            "Generation 34 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1376.5823378464063\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1371.186421488485\tExtraTreesRegressor(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=15, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1369.283585290283\tRandomForestRegressor(CombineDFs(OneHotEncoder(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=7, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1353.8197131518423\tExtraTreesRegressor(ExtraTreesRegressor(XGBRegressor(ZeroCount(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=12, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=5, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_34_idx_1_2021.09.01_10-34-34.py\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_34_idx_3_2021.09.01_10-34-34.py\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "\n",
            "Generation 35 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1376.5823378464063\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1371.186421488485\tExtraTreesRegressor(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=15, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1369.283585290283\tRandomForestRegressor(CombineDFs(OneHotEncoder(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=7, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1353.8197131518423\tExtraTreesRegressor(ExtraTreesRegressor(XGBRegressor(ZeroCount(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=12, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=5, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 56.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
            "\n",
            "Generation 36 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1376.5823378464063\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1371.186421488485\tExtraTreesRegressor(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=15, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1363.9602539391833\tExtraTreesRegressor(CombineDFs(RobustScaler(input_matrix), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1353.8197131518423\tExtraTreesRegressor(ExtraTreesRegressor(XGBRegressor(ZeroCount(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=12, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=5, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_36_idx_2_2021.09.01_10-42-26.py\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 array must not contain infs or NaNs.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _mate_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 [10:42:41] ../src/learner.cc:599: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7f279a74833f]\n",
            "  [bt] (1) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1c4518) [0x7f279a87b518]\n",
            "  [bt] (2) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1d21a3) [0x7f279a8891a3]\n",
            "  [bt] (3) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1b9829) [0x7f279a870829]\n",
            "  [bt] (4) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f279a737ed0]\n",
            "  [bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f284951adae]\n",
            "  [bt] (6) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7f284951a71f]\n",
            "  [bt] (7) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x28c) [0x7f284972e5ac]\n",
            "  [bt] (8) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x109e3) [0x7f284972d9e3]\n",
            "\n",
            ".\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "\n",
            "Generation 37 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1376.5823378464063\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1371.186421488485\tExtraTreesRegressor(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=15, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1363.9602539391833\tExtraTreesRegressor(CombineDFs(RobustScaler(input_matrix), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1353.8197131518423\tExtraTreesRegressor(ExtraTreesRegressor(XGBRegressor(ZeroCount(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=12, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=5, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _mate_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88.\n",
            "_pre_test decorator: _mate_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "\n",
            "Generation 38 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1376.5823378464063\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1371.186421488485\tExtraTreesRegressor(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=15, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1363.9602539391833\tExtraTreesRegressor(CombineDFs(RobustScaler(input_matrix), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1353.8197131518423\tExtraTreesRegressor(ExtraTreesRegressor(XGBRegressor(ZeroCount(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=12, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=5, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 [10:50:38] ../src/learner.cc:599: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7f279a74833f]\n",
            "  [bt] (1) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1c4518) [0x7f279a87b518]\n",
            "  [bt] (2) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1d21a3) [0x7f279a8891a3]\n",
            "  [bt] (3) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1b9829) [0x7f279a870829]\n",
            "  [bt] (4) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f279a737ed0]\n",
            "  [bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f284951adae]\n",
            "  [bt] (6) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7f284951a71f]\n",
            "  [bt] (7) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x28c) [0x7f284972e5ac]\n",
            "  [bt] (8) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x109e3) [0x7f284972d9e3]\n",
            "\n",
            ".\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 84.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=2 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=3 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 39 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1376.5823378464063\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1371.186421488485\tExtraTreesRegressor(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=15, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1338.884889383057\tExtraTreesRegressor(XGBRegressor(ZeroCount(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_39_idx_2_2021.09.01_10-54-23.py\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 [10:54:27] ../src/learner.cc:599: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7f279a74833f]\n",
            "  [bt] (1) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1c4518) [0x7f279a87b518]\n",
            "  [bt] (2) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1d21a3) [0x7f279a8891a3]\n",
            "  [bt] (3) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1b9829) [0x7f279a870829]\n",
            "  [bt] (4) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f279a737ed0]\n",
            "  [bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f284951adae]\n",
            "  [bt] (6) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7f284951a71f]\n",
            "  [bt] (7) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x28c) [0x7f284972e5ac]\n",
            "  [bt] (8) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x109e3) [0x7f284972d9e3]\n",
            "\n",
            ".\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _mate_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "\n",
            "Generation 40 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1376.5823378464063\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1355.963420737265\tExtraTreesRegressor(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1338.884889383057\tExtraTreesRegressor(XGBRegressor(ZeroCount(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_40_idx_1_2021.09.01_10-58-37.py\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _mate_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 70.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "\n",
            "Generation 41 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1376.5823378464063\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1355.963420737265\tExtraTreesRegressor(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1338.884889383057\tExtraTreesRegressor(XGBRegressor(ZeroCount(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "\n",
            "Generation 42 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1376.5823378464063\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1355.963420737265\tExtraTreesRegressor(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1338.884889383057\tExtraTreesRegressor(XGBRegressor(ZeroCount(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "\n",
            "Generation 43 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1376.5823378464063\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1355.963420737265\tExtraTreesRegressor(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1338.884889383057\tExtraTreesRegressor(XGBRegressor(ZeroCount(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 [11:09:45] ../src/learner.cc:599: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7f279a74833f]\n",
            "  [bt] (1) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1c4518) [0x7f279a87b518]\n",
            "  [bt] (2) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1d21a3) [0x7f279a8891a3]\n",
            "  [bt] (3) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1b9829) [0x7f279a870829]\n",
            "  [bt] (4) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f279a737ed0]\n",
            "  [bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f284951adae]\n",
            "  [bt] (6) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7f284951a71f]\n",
            "  [bt] (7) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x28c) [0x7f284972e5ac]\n",
            "  [bt] (8) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x109e3) [0x7f284972d9e3]\n",
            "\n",
            ".\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 44 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1376.5823378464063\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1346.3270319332364\tExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1338.884889383057\tExtraTreesRegressor(XGBRegressor(ZeroCount(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_44_idx_1_2021.09.01_11-12-54.py\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 81.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 45 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1376.5823378464063\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1346.3270319332364\tExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1338.884889383057\tExtraTreesRegressor(XGBRegressor(ZeroCount(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
            "_pre_test decorator: _mate_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 46 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1338.364787236155\tExtraTreesRegressor(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=14, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-5\t-1335.8719408235481\tExtraTreesRegressor(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), StandardScaler(OneHotEncoder(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_46_idx_0_2021.09.01_11-20-10.py\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_46_idx_1_2021.09.01_11-20-10.py\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_46_idx_2_2021.09.01_11-20-10.py\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "\n",
            "Generation 47 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1338.364787236155\tExtraTreesRegressor(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=14, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-5\t-1335.8719408235481\tExtraTreesRegressor(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), StandardScaler(OneHotEncoder(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 [11:24:06] ../src/learner.cc:599: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7f279a74833f]\n",
            "  [bt] (1) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1c4518) [0x7f279a87b518]\n",
            "  [bt] (2) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1d21a3) [0x7f279a8891a3]\n",
            "  [bt] (3) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1b9829) [0x7f279a870829]\n",
            "  [bt] (4) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f279a737ed0]\n",
            "  [bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f284951adae]\n",
            "  [bt] (6) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7f284951a71f]\n",
            "  [bt] (7) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x28c) [0x7f284972e5ac]\n",
            "  [bt] (8) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x109e3) [0x7f284972d9e3]\n",
            "\n",
            ".\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 48 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1338.364787236155\tExtraTreesRegressor(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=14, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1336.360296867474\tExtraTreesRegressor(CombineDFs(input_matrix, SelectPercentile(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), SelectPercentile__percentile=79)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=15, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-5\t-1335.8719408235481\tExtraTreesRegressor(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), StandardScaler(OneHotEncoder(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_48_idx_2_2021.09.01_11-27-43.py\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=2 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
            "\n",
            "Generation 49 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1335.5326231581857\tExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=14, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_49_idx_1_2021.09.01_11-31-15.py\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 [11:31:17] ../src/learner.cc:599: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7f279a74833f]\n",
            "  [bt] (1) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1c4518) [0x7f279a87b518]\n",
            "  [bt] (2) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1d21a3) [0x7f279a8891a3]\n",
            "  [bt] (3) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1b9829) [0x7f279a870829]\n",
            "  [bt] (4) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f279a737ed0]\n",
            "  [bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f284951adae]\n",
            "  [bt] (6) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7f284951a71f]\n",
            "  [bt] (7) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x28c) [0x7f284972e5ac]\n",
            "  [bt] (8) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x109e3) [0x7f284972d9e3]\n",
            "\n",
            ".\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 67.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 64.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 [11:31:28] ../src/learner.cc:599: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7f279a74833f]\n",
            "  [bt] (1) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1c4518) [0x7f279a87b518]\n",
            "  [bt] (2) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1d21a3) [0x7f279a8891a3]\n",
            "  [bt] (3) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1b9829) [0x7f279a870829]\n",
            "  [bt] (4) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f279a737ed0]\n",
            "  [bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f284951adae]\n",
            "  [bt] (6) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7f284951a71f]\n",
            "  [bt] (7) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x28c) [0x7f284972e5ac]\n",
            "  [bt] (8) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x109e3) [0x7f284972d9e3]\n",
            "\n",
            ".\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "\n",
            "Generation 50 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1335.5326231581857\tExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=14, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "\n",
            "Generation 51 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1335.5326231581857\tExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=14, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 [11:38:28] ../src/learner.cc:599: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7f279a74833f]\n",
            "  [bt] (1) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1c4518) [0x7f279a87b518]\n",
            "  [bt] (2) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1d21a3) [0x7f279a8891a3]\n",
            "  [bt] (3) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1b9829) [0x7f279a870829]\n",
            "  [bt] (4) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f279a737ed0]\n",
            "  [bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f284951adae]\n",
            "  [bt] (6) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7f284951a71f]\n",
            "  [bt] (7) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x28c) [0x7f284972e5ac]\n",
            "  [bt] (8) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x109e3) [0x7f284972d9e3]\n",
            "\n",
            ".\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=2 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "\n",
            "Generation 52 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1335.5326231581857\tExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=14, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1328.0088889955928\tExtraTreesRegressor(RobustScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_52_idx_2_2021.09.01_11-41-33.py\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 87.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 53 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1335.5326231581857\tExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=14, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1328.0088889955928\tExtraTreesRegressor(RobustScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=2 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=3 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
            "\n",
            "Generation 54 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1335.5326231581857\tExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=14, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1328.0088889955928\tExtraTreesRegressor(RobustScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 55 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1335.5326231581857\tExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=14, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1328.0088889955928\tExtraTreesRegressor(RobustScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "\n",
            "Generation 56 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1335.5326231581857\tExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=14, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1328.0088889955928\tExtraTreesRegressor(RobustScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _mate_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 [11:54:54] ../src/learner.cc:599: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7f279a74833f]\n",
            "  [bt] (1) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1c4518) [0x7f279a87b518]\n",
            "  [bt] (2) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1d21a3) [0x7f279a8891a3]\n",
            "  [bt] (3) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1b9829) [0x7f279a870829]\n",
            "  [bt] (4) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f279a737ed0]\n",
            "  [bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f284951adae]\n",
            "  [bt] (6) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7f284951a71f]\n",
            "  [bt] (7) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x28c) [0x7f284972e5ac]\n",
            "  [bt] (8) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x109e3) [0x7f284972d9e3]\n",
            "\n",
            ".\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=2 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 57 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1335.5326231581857\tExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=14, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1328.0088889955928\tExtraTreesRegressor(RobustScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 [11:58:46] ../src/learner.cc:599: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7f279a74833f]\n",
            "  [bt] (1) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1c4518) [0x7f279a87b518]\n",
            "  [bt] (2) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1d21a3) [0x7f279a8891a3]\n",
            "  [bt] (3) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1b9829) [0x7f279a870829]\n",
            "  [bt] (4) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f279a737ed0]\n",
            "  [bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f284951adae]\n",
            "  [bt] (6) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7f284951a71f]\n",
            "  [bt] (7) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x28c) [0x7f284972e5ac]\n",
            "  [bt] (8) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x109e3) [0x7f284972d9e3]\n",
            "\n",
            ".\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 89.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 58 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1335.5326231581857\tExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=14, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1328.0088889955928\tExtraTreesRegressor(RobustScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _mate_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _mate_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _mate_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 59 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1335.5326231581857\tExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=14, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1328.0088889955928\tExtraTreesRegressor(RobustScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=2 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 89.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "\n",
            "Generation 60 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1335.5326231581857\tExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=14, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1328.0088889955928\tExtraTreesRegressor(RobustScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1326.0707962131216\tExtraTreesRegressor(ZeroCount(XGBRegressor(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.005), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_60_idx_3_2021.09.01_12-09-48.py\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 61 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1335.4460116317646\tExtraTreesRegressor(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=15, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1328.0088889955928\tExtraTreesRegressor(RobustScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1326.0707962131216\tExtraTreesRegressor(ZeroCount(XGBRegressor(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.005), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_61_idx_1_2021.09.01_12-13-29.py\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 62 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1335.4460116317646\tExtraTreesRegressor(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=15, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1328.0088889955928\tExtraTreesRegressor(RobustScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1326.0707962131216\tExtraTreesRegressor(ZeroCount(XGBRegressor(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.005), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 63 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1335.4460116317646\tExtraTreesRegressor(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=15, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1328.0088889955928\tExtraTreesRegressor(RobustScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1326.0707962131216\tExtraTreesRegressor(ZeroCount(XGBRegressor(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.005), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 61.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _mate_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 62.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "\n",
            "Generation 64 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1335.4460116317646\tExtraTreesRegressor(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=15, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1328.0088889955928\tExtraTreesRegressor(RobustScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1326.0707962131216\tExtraTreesRegressor(ZeroCount(XGBRegressor(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.005), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 65 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1335.4460116317646\tExtraTreesRegressor(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=15, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1328.0088889955928\tExtraTreesRegressor(RobustScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1326.0707962131216\tExtraTreesRegressor(ZeroCount(XGBRegressor(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.005), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 66 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1335.4460116317646\tExtraTreesRegressor(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=15, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1328.0088889955928\tExtraTreesRegressor(RobustScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1314.5585657901906\tExtraTreesRegressor(CombineDFs(ZeroCount(input_matrix), MinMaxScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=15, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_66_idx_3_2021.09.01_12-33-20.py\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _mate_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 67 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1335.4460116317646\tExtraTreesRegressor(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=15, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1328.0088889955928\tExtraTreesRegressor(RobustScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1314.5585657901906\tExtraTreesRegressor(CombineDFs(ZeroCount(input_matrix), MinMaxScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=15, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 86.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 [12:36:55] ../src/learner.cc:599: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7f279a74833f]\n",
            "  [bt] (1) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1c4518) [0x7f279a87b518]\n",
            "  [bt] (2) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1d21a3) [0x7f279a8891a3]\n",
            "  [bt] (3) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1b9829) [0x7f279a870829]\n",
            "  [bt] (4) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f279a737ed0]\n",
            "  [bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f284951adae]\n",
            "  [bt] (6) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7f284951a71f]\n",
            "  [bt] (7) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x28c) [0x7f284972e5ac]\n",
            "  [bt] (8) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x109e3) [0x7f284972d9e3]\n",
            "\n",
            ".\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "\n",
            "Generation 68 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1335.4460116317646\tExtraTreesRegressor(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=15, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1328.0088889955928\tExtraTreesRegressor(RobustScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1314.5585657901906\tExtraTreesRegressor(CombineDFs(ZeroCount(input_matrix), MinMaxScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=15, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "\n",
            "Generation 69 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1335.4460116317646\tExtraTreesRegressor(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=15, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1323.9175422315197\tExtraTreesRegressor(CombineDFs(ZeroCount(input_matrix), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=15, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1314.5585657901906\tExtraTreesRegressor(CombineDFs(ZeroCount(input_matrix), MinMaxScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=15, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_69_idx_2_2021.09.01_12-44-31.py\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91.\n",
            "\n",
            "Generation 70 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1335.4460116317646\tExtraTreesRegressor(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=15, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1323.9175422315197\tExtraTreesRegressor(CombineDFs(ZeroCount(input_matrix), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=15, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1314.5585657901906\tExtraTreesRegressor(CombineDFs(ZeroCount(input_matrix), MinMaxScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=15, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=2 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 [12:48:07] ../src/learner.cc:599: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7f279a74833f]\n",
            "  [bt] (1) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1c4518) [0x7f279a87b518]\n",
            "  [bt] (2) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1d21a3) [0x7f279a8891a3]\n",
            "  [bt] (3) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1b9829) [0x7f279a870829]\n",
            "  [bt] (4) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f279a737ed0]\n",
            "  [bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f284951adae]\n",
            "  [bt] (6) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7f284951a71f]\n",
            "  [bt] (7) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x28c) [0x7f284972e5ac]\n",
            "  [bt] (8) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x109e3) [0x7f284972d9e3]\n",
            "\n",
            ".\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "\n",
            "Generation 71 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1335.4460116317646\tExtraTreesRegressor(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=15, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1323.9175422315197\tExtraTreesRegressor(CombineDFs(ZeroCount(input_matrix), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=15, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1314.5585657901906\tExtraTreesRegressor(CombineDFs(ZeroCount(input_matrix), MinMaxScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=15, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 72 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1335.4460116317646\tExtraTreesRegressor(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=15, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1319.6558409231272\tExtraTreesRegressor(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1314.5585657901906\tExtraTreesRegressor(CombineDFs(ZeroCount(input_matrix), MinMaxScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=15, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_72_idx_2_2021.09.01_12-56-00.py\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 86.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 73 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1335.4460116317646\tExtraTreesRegressor(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=15, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1319.6558409231272\tExtraTreesRegressor(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1314.5585657901906\tExtraTreesRegressor(CombineDFs(ZeroCount(input_matrix), MinMaxScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=15, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 [12:59:56] ../src/learner.cc:599: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7f279a74833f]\n",
            "  [bt] (1) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1c4518) [0x7f279a87b518]\n",
            "  [bt] (2) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1d21a3) [0x7f279a8891a3]\n",
            "  [bt] (3) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1b9829) [0x7f279a870829]\n",
            "  [bt] (4) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f279a737ed0]\n",
            "  [bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f284951adae]\n",
            "  [bt] (6) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7f284951a71f]\n",
            "  [bt] (7) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x28c) [0x7f284972e5ac]\n",
            "  [bt] (8) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x109e3) [0x7f284972d9e3]\n",
            "\n",
            ".\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.\n",
            "Invalid pipeline encountered. Skipping its evaluation.\n",
            "\n",
            "Generation 74 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1335.4460116317646\tExtraTreesRegressor(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=15, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1319.6558409231272\tExtraTreesRegressor(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1314.5585657901906\tExtraTreesRegressor(CombineDFs(ZeroCount(input_matrix), MinMaxScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=15, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 87.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=2 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=2 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 75 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1335.4460116317646\tExtraTreesRegressor(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=15, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1319.6558409231272\tExtraTreesRegressor(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1314.5585657901906\tExtraTreesRegressor(CombineDFs(ZeroCount(input_matrix), MinMaxScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=15, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=2 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "Invalid pipeline encountered. Skipping its evaluation.\n",
            "\n",
            "Generation 76 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1335.4460116317646\tExtraTreesRegressor(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=15, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1319.6558409231272\tExtraTreesRegressor(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1314.5585657901906\tExtraTreesRegressor(CombineDFs(ZeroCount(input_matrix), MinMaxScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=15, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 [13:11:38] ../src/learner.cc:599: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7f279a74833f]\n",
            "  [bt] (1) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1c4518) [0x7f279a87b518]\n",
            "  [bt] (2) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1d21a3) [0x7f279a8891a3]\n",
            "  [bt] (3) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1b9829) [0x7f279a870829]\n",
            "  [bt] (4) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f279a737ed0]\n",
            "  [bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f284951adae]\n",
            "  [bt] (6) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7f284951a71f]\n",
            "  [bt] (7) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x28c) [0x7f284972e5ac]\n",
            "  [bt] (8) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x109e3) [0x7f284972d9e3]\n",
            "\n",
            ".\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 77 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1335.4460116317646\tExtraTreesRegressor(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=15, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1319.6558409231272\tExtraTreesRegressor(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1314.5585657901906\tExtraTreesRegressor(CombineDFs(ZeroCount(input_matrix), MinMaxScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=15, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 [13:15:09] ../src/learner.cc:599: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7f279a74833f]\n",
            "  [bt] (1) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1c4518) [0x7f279a87b518]\n",
            "  [bt] (2) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1d21a3) [0x7f279a8891a3]\n",
            "  [bt] (3) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1b9829) [0x7f279a870829]\n",
            "  [bt] (4) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f279a737ed0]\n",
            "  [bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f284951adae]\n",
            "  [bt] (6) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7f284951a71f]\n",
            "  [bt] (7) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x28c) [0x7f284972e5ac]\n",
            "  [bt] (8) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x109e3) [0x7f284972d9e3]\n",
            "\n",
            ".\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 [13:15:17] ../src/learner.cc:599: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7f279a74833f]\n",
            "  [bt] (1) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1c4518) [0x7f279a87b518]\n",
            "  [bt] (2) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1d21a3) [0x7f279a8891a3]\n",
            "  [bt] (3) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1b9829) [0x7f279a870829]\n",
            "  [bt] (4) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f279a737ed0]\n",
            "  [bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f284951adae]\n",
            "  [bt] (6) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7f284951a71f]\n",
            "  [bt] (7) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x28c) [0x7f284972e5ac]\n",
            "  [bt] (8) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x109e3) [0x7f284972d9e3]\n",
            "\n",
            ".\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 78 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1333.4058155318912\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=19, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1319.6558409231272\tExtraTreesRegressor(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1314.5585657901906\tExtraTreesRegressor(CombineDFs(ZeroCount(input_matrix), MinMaxScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=15, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_78_idx_1_2021.09.01_13-18-30.py\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 57.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 [13:18:33] ../src/learner.cc:599: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7f279a74833f]\n",
            "  [bt] (1) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1c4518) [0x7f279a87b518]\n",
            "  [bt] (2) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1d21a3) [0x7f279a8891a3]\n",
            "  [bt] (3) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1b9829) [0x7f279a870829]\n",
            "  [bt] (4) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f279a737ed0]\n",
            "  [bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f284951adae]\n",
            "  [bt] (6) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7f284951a71f]\n",
            "  [bt] (7) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x28c) [0x7f284972e5ac]\n",
            "  [bt] (8) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x109e3) [0x7f284972d9e3]\n",
            "\n",
            ".\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 79 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1333.4058155318912\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=19, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1319.6558409231272\tExtraTreesRegressor(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1314.5585657901906\tExtraTreesRegressor(CombineDFs(ZeroCount(input_matrix), MinMaxScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=15, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-6\t-1290.5446801522107\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder(XGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_79_idx_4_2021.09.01_13-21-50.py\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "Invalid pipeline encountered. Skipping its evaluation.\n",
            "\n",
            "Generation 80 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1333.4058155318912\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=19, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1319.6558409231272\tExtraTreesRegressor(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1314.5585657901906\tExtraTreesRegressor(CombineDFs(ZeroCount(input_matrix), MinMaxScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=15, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-5\t-1304.7317317256711\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-6\t-1290.5446801522107\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder(XGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_80_idx_4_2021.09.01_13-24-56.py\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _mate_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "\n",
            "Generation 81 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1333.4058155318912\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=19, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1319.6558409231272\tExtraTreesRegressor(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1314.5585657901906\tExtraTreesRegressor(CombineDFs(ZeroCount(input_matrix), MinMaxScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=15, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-5\t-1304.7317317256711\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-6\t-1290.5446801522107\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder(XGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 89.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 82 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1333.4058155318912\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=19, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1319.6558409231272\tExtraTreesRegressor(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1314.5585657901906\tExtraTreesRegressor(CombineDFs(ZeroCount(input_matrix), MinMaxScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=15, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-5\t-1304.7317317256711\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-6\t-1290.5446801522107\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder(XGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 [13:32:44] ../src/learner.cc:599: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7f279a74833f]\n",
            "  [bt] (1) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1c4518) [0x7f279a87b518]\n",
            "  [bt] (2) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1d21a3) [0x7f279a8891a3]\n",
            "  [bt] (3) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1b9829) [0x7f279a870829]\n",
            "  [bt] (4) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f279a737ed0]\n",
            "  [bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f284951adae]\n",
            "  [bt] (6) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7f284951a71f]\n",
            "  [bt] (7) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x28c) [0x7f284972e5ac]\n",
            "  [bt] (8) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x109e3) [0x7f284972d9e3]\n",
            "\n",
            ".\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 83 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1333.4058155318912\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=19, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1319.6558409231272\tExtraTreesRegressor(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1305.5676737920141\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-5\t-1304.7317317256711\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-6\t-1290.5446801522107\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder(XGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_83_idx_3_2021.09.01_13-36-23.py\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 84 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1333.4058155318912\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=19, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1319.6558409231272\tExtraTreesRegressor(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1305.5676737920141\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-5\t-1304.7317317256711\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-6\t-1290.5446801522107\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder(XGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 [13:41:04] ../src/learner.cc:599: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7f279a74833f]\n",
            "  [bt] (1) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1c4518) [0x7f279a87b518]\n",
            "  [bt] (2) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1d21a3) [0x7f279a8891a3]\n",
            "  [bt] (3) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1b9829) [0x7f279a870829]\n",
            "  [bt] (4) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f279a737ed0]\n",
            "  [bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f284951adae]\n",
            "  [bt] (6) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7f284951a71f]\n",
            "  [bt] (7) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x28c) [0x7f284972e5ac]\n",
            "  [bt] (8) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x109e3) [0x7f284972d9e3]\n",
            "\n",
            ".\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 85 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1333.4058155318912\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=19, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1319.6558409231272\tExtraTreesRegressor(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1305.5676737920141\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-5\t-1304.7317317256711\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-6\t-1290.5446801522107\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder(XGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 86 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1333.4058155318912\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=19, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1319.6558409231272\tExtraTreesRegressor(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1305.5676737920141\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-5\t-1304.7317317256711\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-6\t-1290.5446801522107\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder(XGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 [13:47:59] ../src/learner.cc:599: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7f279a74833f]\n",
            "  [bt] (1) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1c4518) [0x7f279a87b518]\n",
            "  [bt] (2) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1d21a3) [0x7f279a8891a3]\n",
            "  [bt] (3) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1b9829) [0x7f279a870829]\n",
            "  [bt] (4) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f279a737ed0]\n",
            "  [bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f284951adae]\n",
            "  [bt] (6) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7f284951a71f]\n",
            "  [bt] (7) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x28c) [0x7f284972e5ac]\n",
            "  [bt] (8) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x109e3) [0x7f284972d9e3]\n",
            "\n",
            ".\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 90.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 [13:48:09] ../src/learner.cc:599: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7f279a74833f]\n",
            "  [bt] (1) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1c4518) [0x7f279a87b518]\n",
            "  [bt] (2) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1d21a3) [0x7f279a8891a3]\n",
            "  [bt] (3) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1b9829) [0x7f279a870829]\n",
            "  [bt] (4) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f279a737ed0]\n",
            "  [bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f284951adae]\n",
            "  [bt] (6) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7f284951a71f]\n",
            "  [bt] (7) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x28c) [0x7f284972e5ac]\n",
            "  [bt] (8) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x109e3) [0x7f284972d9e3]\n",
            "\n",
            ".\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "\n",
            "Generation 87 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1333.4058155318912\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=19, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1319.6558409231272\tExtraTreesRegressor(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1305.5676737920141\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-5\t-1298.947750543802\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-6\t-1290.5446801522107\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder(XGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_87_idx_4_2021.09.01_13-51-49.py\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 [13:51:49] ../src/learner.cc:599: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7f279a74833f]\n",
            "  [bt] (1) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1c4518) [0x7f279a87b518]\n",
            "  [bt] (2) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1d21a3) [0x7f279a8891a3]\n",
            "  [bt] (3) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1b9829) [0x7f279a870829]\n",
            "  [bt] (4) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f279a737ed0]\n",
            "  [bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f284951adae]\n",
            "  [bt] (6) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7f284951a71f]\n",
            "  [bt] (7) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x28c) [0x7f284972e5ac]\n",
            "  [bt] (8) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x109e3) [0x7f284972d9e3]\n",
            "\n",
            ".\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 [13:51:51] ../src/learner.cc:599: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7f279a74833f]\n",
            "  [bt] (1) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1c4518) [0x7f279a87b518]\n",
            "  [bt] (2) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1d21a3) [0x7f279a8891a3]\n",
            "  [bt] (3) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1b9829) [0x7f279a870829]\n",
            "  [bt] (4) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f279a737ed0]\n",
            "  [bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f284951adae]\n",
            "  [bt] (6) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7f284951a71f]\n",
            "  [bt] (7) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x28c) [0x7f284972e5ac]\n",
            "  [bt] (8) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x109e3) [0x7f284972d9e3]\n",
            "\n",
            ".\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 Cosine affinity cannot be used when X contains zero vectors.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 [13:51:56] ../src/learner.cc:599: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7f279a74833f]\n",
            "  [bt] (1) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1c4518) [0x7f279a87b518]\n",
            "  [bt] (2) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1d21a3) [0x7f279a8891a3]\n",
            "  [bt] (3) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1b9829) [0x7f279a870829]\n",
            "  [bt] (4) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f279a737ed0]\n",
            "  [bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f284951adae]\n",
            "  [bt] (6) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7f284951a71f]\n",
            "  [bt] (7) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x28c) [0x7f284972e5ac]\n",
            "  [bt] (8) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x109e3) [0x7f284972d9e3]\n",
            "\n",
            ".\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 88 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1333.4058155318912\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=19, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1319.6558409231272\tExtraTreesRegressor(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1305.5676737920141\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-5\t-1298.947750543802\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-6\t-1290.5446801522107\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder(XGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 89 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1333.4058155318912\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=19, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1319.6558409231272\tExtraTreesRegressor(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1305.5676737920141\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-5\t-1296.206013786186\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), ElasticNetCV__l1_ratio=0.25, ElasticNetCV__tol=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-6\t-1290.5446801522107\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder(XGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_89_idx_4_2021.09.01_13-59-35.py\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 75.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 78.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 100.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 [13:59:43] ../src/learner.cc:599: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7f279a74833f]\n",
            "  [bt] (1) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1c4518) [0x7f279a87b518]\n",
            "  [bt] (2) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1d21a3) [0x7f279a8891a3]\n",
            "  [bt] (3) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1b9829) [0x7f279a870829]\n",
            "  [bt] (4) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f279a737ed0]\n",
            "  [bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f284951adae]\n",
            "  [bt] (6) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7f284951a71f]\n",
            "  [bt] (7) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x28c) [0x7f284972e5ac]\n",
            "  [bt] (8) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x109e3) [0x7f284972d9e3]\n",
            "\n",
            ".\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 62.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 90 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1333.4058155318912\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=19, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1319.6558409231272\tExtraTreesRegressor(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1302.7579484727175\tExtraTreesRegressor(CombineDFs(CombineDFs(input_matrix, XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-5\t-1296.206013786186\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), ElasticNetCV__l1_ratio=0.25, ElasticNetCV__tol=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-6\t-1290.5446801522107\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder(XGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_90_idx_3_2021.09.01_14-03-18.py\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 79.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 91 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1333.4058155318912\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=19, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1313.4324355796189\tExtraTreesRegressor(CombineDFs(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), input_matrix), CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1302.7579484727175\tExtraTreesRegressor(CombineDFs(CombineDFs(input_matrix, XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-5\t-1296.206013786186\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), ElasticNetCV__l1_ratio=0.25, ElasticNetCV__tol=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-6\t-1290.5446801522107\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder(XGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_91_idx_2_2021.09.01_14-07-18.py\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 [14:07:26] ../src/learner.cc:599: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7f279a74833f]\n",
            "  [bt] (1) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1c4518) [0x7f279a87b518]\n",
            "  [bt] (2) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1d21a3) [0x7f279a8891a3]\n",
            "  [bt] (3) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1b9829) [0x7f279a870829]\n",
            "  [bt] (4) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f279a737ed0]\n",
            "  [bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f284951adae]\n",
            "  [bt] (6) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7f284951a71f]\n",
            "  [bt] (7) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x28c) [0x7f284972e5ac]\n",
            "  [bt] (8) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x109e3) [0x7f284972d9e3]\n",
            "\n",
            ".\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 92 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1333.4058155318912\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=19, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1311.8459157791772\tExtraTreesRegressor(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), input_matrix)), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1302.6583669276545\tExtraTreesRegressor(CombineDFs(GradientBoostingRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.8500000000000001, GradientBoostingRegressor__min_samples_leaf=7, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-5\t-1296.206013786186\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), ElasticNetCV__l1_ratio=0.25, ElasticNetCV__tol=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-6\t-1290.5446801522107\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder(XGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_92_idx_2_2021.09.01_14-11-07.py\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_92_idx_3_2021.09.01_14-11-07.py\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 75.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 93 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1317.5701606273358\tExtraTreesRegressor(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1311.8459157791772\tExtraTreesRegressor(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), input_matrix)), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1302.6583669276545\tExtraTreesRegressor(CombineDFs(GradientBoostingRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.8500000000000001, GradientBoostingRegressor__min_samples_leaf=7, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-5\t-1296.206013786186\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), ElasticNetCV__l1_ratio=0.25, ElasticNetCV__tol=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-6\t-1290.5446801522107\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder(XGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_93_idx_1_2021.09.01_14-14-45.py\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 87.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 94 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1317.5701606273358\tExtraTreesRegressor(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1311.8459157791772\tExtraTreesRegressor(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), input_matrix)), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1302.6583669276545\tExtraTreesRegressor(CombineDFs(GradientBoostingRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.8500000000000001, GradientBoostingRegressor__min_samples_leaf=7, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-5\t-1296.206013786186\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), ElasticNetCV__l1_ratio=0.25, ElasticNetCV__tol=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-6\t-1290.5446801522107\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder(XGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 56.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 95 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1317.5701606273358\tExtraTreesRegressor(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1311.8459157791772\tExtraTreesRegressor(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), input_matrix)), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1302.6583669276545\tExtraTreesRegressor(CombineDFs(GradientBoostingRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.8500000000000001, GradientBoostingRegressor__min_samples_leaf=7, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-5\t-1296.206013786186\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), ElasticNetCV__l1_ratio=0.25, ElasticNetCV__tol=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-6\t-1290.5446801522107\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder(XGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-7\t-1286.7439321696297\tExtraTreesRegressor(CombineDFs(XGBRegressor(MaxAbsScaler(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=12, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder(XGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_95_idx_6_2021.09.01_14-23-04.py\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 [14:23:04] ../src/learner.cc:599: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7f279a74833f]\n",
            "  [bt] (1) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1c4518) [0x7f279a87b518]\n",
            "  [bt] (2) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1d21a3) [0x7f279a8891a3]\n",
            "  [bt] (3) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1b9829) [0x7f279a870829]\n",
            "  [bt] (4) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f279a737ed0]\n",
            "  [bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f284951adae]\n",
            "  [bt] (6) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7f284951a71f]\n",
            "  [bt] (7) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x28c) [0x7f284972e5ac]\n",
            "  [bt] (8) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x109e3) [0x7f284972d9e3]\n",
            "\n",
            ".\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 89.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 90.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 96 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1317.5701606273358\tExtraTreesRegressor(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1311.8459157791772\tExtraTreesRegressor(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), input_matrix)), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1302.6583669276545\tExtraTreesRegressor(CombineDFs(GradientBoostingRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.8500000000000001, GradientBoostingRegressor__min_samples_leaf=7, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-5\t-1296.206013786186\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), ElasticNetCV__l1_ratio=0.25, ElasticNetCV__tol=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-6\t-1290.5446801522107\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder(XGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-7\t-1286.7439321696297\tExtraTreesRegressor(CombineDFs(XGBRegressor(MaxAbsScaler(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=12, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder(XGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 90.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 97 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1317.5701606273358\tExtraTreesRegressor(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1307.853628404347\tExtraTreesRegressor(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1302.6583669276545\tExtraTreesRegressor(CombineDFs(GradientBoostingRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.8500000000000001, GradientBoostingRegressor__min_samples_leaf=7, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-5\t-1295.5601199421503\tExtraTreesRegressor(CombineDFs(CombineDFs(input_matrix, XGBRegressor(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-6\t-1290.5446801522107\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder(XGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-7\t-1286.7439321696297\tExtraTreesRegressor(CombineDFs(XGBRegressor(MaxAbsScaler(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=12, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder(XGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_97_idx_2_2021.09.01_14-31-23.py\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_97_idx_4_2021.09.01_14-31-23.py\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 [14:31:25] ../src/learner.cc:599: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7f279a74833f]\n",
            "  [bt] (1) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1c4518) [0x7f279a87b518]\n",
            "  [bt] (2) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1d21a3) [0x7f279a8891a3]\n",
            "  [bt] (3) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1b9829) [0x7f279a870829]\n",
            "  [bt] (4) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f279a737ed0]\n",
            "  [bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f284951adae]\n",
            "  [bt] (6) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7f284951a71f]\n",
            "  [bt] (7) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x28c) [0x7f284972e5ac]\n",
            "  [bt] (8) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x109e3) [0x7f284972d9e3]\n",
            "\n",
            ".\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 [14:31:33] ../src/learner.cc:599: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7f279a74833f]\n",
            "  [bt] (1) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1c4518) [0x7f279a87b518]\n",
            "  [bt] (2) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1d21a3) [0x7f279a8891a3]\n",
            "  [bt] (3) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1b9829) [0x7f279a870829]\n",
            "  [bt] (4) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f279a737ed0]\n",
            "  [bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f284951adae]\n",
            "  [bt] (6) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7f284951a71f]\n",
            "  [bt] (7) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x28c) [0x7f284972e5ac]\n",
            "  [bt] (8) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x109e3) [0x7f284972d9e3]\n",
            "\n",
            ".\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 [14:31:36] ../src/learner.cc:599: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7f279a74833f]\n",
            "  [bt] (1) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1c4518) [0x7f279a87b518]\n",
            "  [bt] (2) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1d21a3) [0x7f279a8891a3]\n",
            "  [bt] (3) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1b9829) [0x7f279a870829]\n",
            "  [bt] (4) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f279a737ed0]\n",
            "  [bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f284951adae]\n",
            "  [bt] (6) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7f284951a71f]\n",
            "  [bt] (7) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x28c) [0x7f284972e5ac]\n",
            "  [bt] (8) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x109e3) [0x7f284972d9e3]\n",
            "\n",
            ".\n",
            "_pre_test decorator: _random_mutation_operator: num_test=1 '(slice(None, None, None), 0)' is an invalid key.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 98 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1317.5701606273358\tExtraTreesRegressor(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1307.853628404347\tExtraTreesRegressor(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1302.6583669276545\tExtraTreesRegressor(CombineDFs(GradientBoostingRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.8500000000000001, GradientBoostingRegressor__min_samples_leaf=7, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-5\t-1295.5601199421503\tExtraTreesRegressor(CombineDFs(CombineDFs(input_matrix, XGBRegressor(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-6\t-1290.5446801522107\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder(XGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-7\t-1286.7439321696297\tExtraTreesRegressor(CombineDFs(XGBRegressor(MaxAbsScaler(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=12, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder(XGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 99.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 87.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 99 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1317.5701606273358\tExtraTreesRegressor(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1307.853628404347\tExtraTreesRegressor(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1294.8383083011722\tExtraTreesRegressor(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), XGBRegressor(MinMaxScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=14, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-6\t-1290.5446801522107\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder(XGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-7\t-1286.7439321696297\tExtraTreesRegressor(CombineDFs(XGBRegressor(MaxAbsScaler(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=12, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder(XGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_99_idx_3_2021.09.01_14-39-10.py\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 [14:39:12] ../src/learner.cc:599: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7f279a74833f]\n",
            "  [bt] (1) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1c4518) [0x7f279a87b518]\n",
            "  [bt] (2) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1d21a3) [0x7f279a8891a3]\n",
            "  [bt] (3) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(+0x1b9829) [0x7f279a870829]\n",
            "  [bt] (4) /usr/local/lib/python3.7/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f279a737ed0]\n",
            "  [bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f284951adae]\n",
            "  [bt] (6) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7f284951a71f]\n",
            "  [bt] (7) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x28c) [0x7f284972e5ac]\n",
            "  [bt] (8) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x109e3) [0x7f284972d9e3]\n",
            "\n",
            ".\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
            "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 73.\n",
            "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
            "\n",
            "Generation 100 - Current Pareto front scores:\n",
            "\n",
            "-1\t-1374.0351433579024\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
            "\n",
            "-2\t-1317.5701606273358\tExtraTreesRegressor(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-3\t-1307.853628404347\tExtraTreesRegressor(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-4\t-1294.8383083011722\tExtraTreesRegressor(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), XGBRegressor(MinMaxScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=14, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-5\t-1294.002749007442\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=12, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), CombineDFs(input_matrix, AdaBoostRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.5, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-6\t-1290.5446801522107\tExtraTreesRegressor(CombineDFs(XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder(XGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "\n",
            "-7\t-1286.7439321696297\tExtraTreesRegressor(CombineDFs(XGBRegressor(MaxAbsScaler(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=12, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0), OneHotEncoder(XGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Saving periodic pipeline from pareto front to tpot_results_no.txt/pipeline_gen_100_idx_4_2021.09.01_14-43-15.py\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "Periodic pipeline was not saved, probably saved before...\n",
            "-1305.714257339371\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tXoRbrT6gkK"
      },
      "source": [
        "X_test_scaled = X_test[select]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiRxg_bSGgVy"
      },
      "source": [
        "pred= tpot.predict(X_test)\n",
        "sub=pd.read_csv('/content/drive/Shareddrives/머신러닝 스터디 2조/따릉이 데이콘/데이터 전처리 완료/submission.csv')\n",
        "sub['count']=pred\n",
        "sub.to_csv('/content/drive/MyDrive/KUBIG/ML 2021-SUMMER/따릉이/submission_tpot_iqr.csv',index=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfIdj4q00qiQ"
      },
      "source": [
        "exctracted_best_model = tpot.fitted_pipeline_.steps[-1][1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ME2W8RD-2ROP",
        "outputId": "533bc6a7-4062-407b-b38a-102c4b8a70ca"
      },
      "source": [
        "exctracted_best_model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
              "                    max_depth=None, max_features=0.45, max_leaf_nodes=None,\n",
              "                    max_samples=None, min_impurity_decrease=0.0,\n",
              "                    min_impurity_split=None, min_samples_leaf=7,\n",
              "                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
              "                    n_estimators=100, n_jobs=None, oob_score=False,\n",
              "                    random_state=None, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkvRhe5U2TBR"
      },
      "source": [
        "feature_importance = exctracted_best_model.feature_importances_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzZewYHE6x9-",
        "outputId": "f197d96e-7ac9-49bd-f5bc-9b39b1cb814f"
      },
      "source": [
        "importance = {}\n",
        "for feature_name, feature_score in zip(X_train.columns, feature_importance):\n",
        "    print(feature_name, '\\t', feature_score)\n",
        "    importance[feature_name]= feature_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hour \t 0.2902708806186832\n",
            "hour_bef_temperature \t 0.045219985425877664\n",
            "hour_bef_precipitation \t 0.03131576790755645\n",
            "hour_bef_windspeed \t 0.0027616747936715464\n",
            "hour_bef_humidity \t 0.0004158107290248483\n",
            "hour_bef_visibility \t 0.0027708453445040903\n",
            "hour_bef_ozone \t 0.0007117242622697055\n",
            "hour_bef_pm10 \t 0.0038861025807509697\n",
            "hour_bef_pm2.5 \t 0.00047553606159542606\n",
            "t_windspeed \t 0.0003708780475166772\n",
            "t_pm10 \t 0.000829232345061954\n",
            "t_pm2.5 \t 0.0006255032486595448\n",
            "hour_sin \t 0.0003771633810296932\n",
            "hour_category \t 0.09749834007649336\n",
            "hour_category_1 \t 0.006296372230054265\n",
            "hour_category_2 \t 0.0012733421721986627\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3dymo7c-eHS",
        "outputId": "76b03921-9881-4a7e-8772-98ab4c2ec7ad"
      },
      "source": [
        "importance"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'hour': 0.2902708806186832,\n",
              " 'hour_bef_humidity': 0.0004158107290248483,\n",
              " 'hour_bef_ozone': 0.0007117242622697055,\n",
              " 'hour_bef_pm10': 0.0038861025807509697,\n",
              " 'hour_bef_pm2.5': 0.00047553606159542606,\n",
              " 'hour_bef_precipitation': 0.03131576790755645,\n",
              " 'hour_bef_temperature': 0.045219985425877664,\n",
              " 'hour_bef_visibility': 0.0027708453445040903,\n",
              " 'hour_bef_windspeed': 0.0027616747936715464,\n",
              " 'hour_category': 0.09749834007649336,\n",
              " 'hour_category_1': 0.006296372230054265,\n",
              " 'hour_category_2': 0.0012733421721986627,\n",
              " 'hour_sin': 0.0003771633810296932,\n",
              " 't_pm10': 0.000829232345061954,\n",
              " 't_pm2.5': 0.0006255032486595448,\n",
              " 't_windspeed': 0.0003708780475166772}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmStELB3_JmL"
      },
      "source": [
        "s  = Series(importance,index=importance.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AV2Qv0H-APA3"
      },
      "source": [
        "importance_df = pd.DataFrame(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_6hazRoBCzu"
      },
      "source": [
        "importance_df.columns = ['score']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "id": "bjRBN9YkAkjE",
        "outputId": "70562162-70ba-4c1f-ddf1-c0f30c0d9302"
      },
      "source": [
        "importance_df.sort_values('score',ascending = False)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>hour</th>\n",
              "      <td>0.290271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hour_category</th>\n",
              "      <td>0.097498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hour_bef_temperature</th>\n",
              "      <td>0.045220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hour_bef_precipitation</th>\n",
              "      <td>0.031316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hour_category_1</th>\n",
              "      <td>0.006296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hour_bef_pm10</th>\n",
              "      <td>0.003886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hour_bef_visibility</th>\n",
              "      <td>0.002771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hour_bef_windspeed</th>\n",
              "      <td>0.002762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hour_category_2</th>\n",
              "      <td>0.001273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>t_pm10</th>\n",
              "      <td>0.000829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hour_bef_ozone</th>\n",
              "      <td>0.000712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>t_pm2.5</th>\n",
              "      <td>0.000626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hour_bef_pm2.5</th>\n",
              "      <td>0.000476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hour_bef_humidity</th>\n",
              "      <td>0.000416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hour_sin</th>\n",
              "      <td>0.000377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>t_windspeed</th>\n",
              "      <td>0.000371</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                           score\n",
              "hour                    0.290271\n",
              "hour_category           0.097498\n",
              "hour_bef_temperature    0.045220\n",
              "hour_bef_precipitation  0.031316\n",
              "hour_category_1         0.006296\n",
              "hour_bef_pm10           0.003886\n",
              "hour_bef_visibility     0.002771\n",
              "hour_bef_windspeed      0.002762\n",
              "hour_category_2         0.001273\n",
              "t_pm10                  0.000829\n",
              "hour_bef_ozone          0.000712\n",
              "t_pm2.5                 0.000626\n",
              "hour_bef_pm2.5          0.000476\n",
              "hour_bef_humidity       0.000416\n",
              "hour_sin                0.000377\n",
              "t_windspeed             0.000371"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bhoivLX3Xl0"
      },
      "source": [
        "tpot.export('/content/drive/Shareddrives/머신러닝 스터디 2조/따릉이 데이콘/tpot_pipeline.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "craNH7KR4k-v"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}